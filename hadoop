word count 

import java.io.IOException; 
import java.util.StringTokenizer; 
import org.apache.hadoop.conf.Configuration; 
import org.apache.hadoop.fs.Path; 
import org.apache.hadoop.io.IntWritable; 
import org.apache.hadoop.io.Text; 
import org.apache.hadoop.mapreduce.Job; 
import org.apache.hadoop.mapreduce.Mapper; 
import org.apache.hadoop.mapreduce.Reducer; 
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; 
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 
public class WordCount { 
 public static class TokenizerMapper 
 extends Mapper<Object, Text, Text, IntWritable>{ 
 private final static IntWritable one = new IntWritable(1); 
 private Text word = new Text(); 
 public void map(Object key, Text value, Context context 
 ) throws IOException, InterruptedException { 
 StringTokenizer itr = new StringTokenizer(value.toString()); 
 while (itr.hasMoreTokens()) { 
 word.set(itr.nextToken()); 
 context.write(word, one); 
 } 
 } 
 } 
 public static class IntSumReducer 
 extends Reducer<Text,IntWritable,Text,IntWritable> { 
 private IntWritable result = new IntWritable(); 
 public void reduce(Text key, Iterable<IntWritable> values,Context context) throws 
IOException, InterruptedException { 
 int sum = 0;
 for (IntWritable val : values) { 
 sum += val.get(); 
 } 
 result.set(sum); 
 context.write(key, result); 
 } 
 } 
 public static void main(String[] args) throws Exception { 
 Configuration conf = new Configuration(); 
 Job job = Job.getInstance(conf, "word count"); 
 job.setJarByClass(WordCount.class); 
 job.setMapperClass(TokenizerMapper.class); 
 job.setCombinerClass(IntSumReducer.class); 
 job.setReducerClass(IntSumReducer.class); 
 job.setOutputKeyClass(Text.class); 
 job.setOutputValueClass(IntWritable.class); 
 FileInputFormat.addInputPath(job, new Path(args[0])); 
 FileOutputFormat.setOutputPath(job, new Path(args[1])); 
 System.exit(job.waitForCompletion(true) ? 0 : 1); 
 } 
}




weatherdataset 
mapper:-

import java.io.IOException; 
import org.apache.hadoop.io.LongWritable; 
import org.apache.hadoop.io.Text; 
import org.apache.hadoop.io.IntWritable; 
import org.apache.hadoop.mapreduce.Mapper; 
public class MaxTempMapper extends Mapper<LongWritable, Text, Text, IntWritable > {
public void map(LongWritable key, Text value, Context context) 
 throws IOException, InterruptedException { 
 String line=value.toString(); 
 String year=line.substring(15,19); 
 int airtemp; 
 if(line.charAt(87)== '+') { 
 airtemp=Integer.parseInt(line.substring(88,92)); 
 } else 
 airtemp=Integer.parseInt(line.substring(87,92)); 
 String q=line.substring(92,93); 
 if(airtemp!=9999&&q.matches("[01459]")) 
 { 
 context.write(new Text(year),new IntWritable(airtemp)); 
 } 
 } 
 }
 
 
 reducer:-
import java.io.IOException; 
import org.apache.hadoop.io.Text; 
import org.apache.hadoop.io.IntWritable; 
import org.apache.hadoop.mapreduce.Reducer; 
public class MaxTempReducer extends Reducer<Text, IntWritable, Text, IntWritable> { 
public void reduce(Text key, Iterable<IntWritable> values, Context context) 
 throws IOException, InterruptedException { 
 int maxvalue=Integer.MIN_VALUE; 
 for (IntWritable value : values) { 
maxvalue=Math.max(maxvalue, value.get()); 
 } 
 context.write(key, new IntWritable(maxvalue)); 
 } 
}

driver:-
import org.apache.hadoop.conf.Configuration; 
import org.apache.hadoop.io.IntWritable; 
import org.apache.hadoop.fs.Path; 
import org.apache.hadoop.io.Text; 
import org.apache.hadoop.mapreduce.Job; 
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; 
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 
public class MaxTemp { 
 public static void main(String[] args) throws Exception { 
 Configuration conf = new Configuration(); 
 Job job = Job.getInstance(conf, "MaxTemp"); 
 job.setJarByClass(MaxTemp.class); 
 // TODO: specify a mapper 
 job.setMapperClass(MaxTempMapper.class); 
 // TODO: specify a reducer 
 job.setReducerClass(MaxTempReducer.class); 
 // TODO: specify output types 
 job.setOutputKeyClass(Text.class); 
 job.setOutputValueClass(IntWritable.class); 
 // TODO: specify input and output DIRECTORIES (not files) 
 FileInputFormat.setInputPaths(job, new Path(args[0])); 
 FileOutputFormat.setOutputPath(job, new Path(args[1])); 
 if (!job.waitForCompletion(true)) 
 return; 
 } 
}
